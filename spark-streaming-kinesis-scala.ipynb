{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming 读取 Kinesis 数据 (scala)\n",
    "\n",
    "Kinesis中的原数据，目前 Kinesis 中不断有数据注入。\n",
    "```json\n",
    "{\n",
    "    \"ordertime\": 1573573055,\n",
    "    \"orderid\": 23,\n",
    "    \"itemid\": \"Item_1231231\",\n",
    "    \"orderunits\": 15,\n",
    "    \"address\": {\n",
    "        \"city\": \"City_a\",\n",
    "        \"state\": \"State_xxx\",\n",
    "        \"zipcode\": 10000\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "最终目标为\n",
    "1. Kinesis 中的数据落到 S3 中\n",
    "2. 根据 **ordertime** 这个字段进行分区，例如 `ordertime=2019101123/`\n",
    "3. 落盘 S3 的数据需要进行铺平, 目标格式如下：\n",
    "```json\n",
    "{\n",
    "    \"ordertime\": 1573573055,\n",
    "    \"orderid\": 23,\n",
    "    \"itemid\": \"Item_1231231\",\n",
    "    \"orderunits\": 15,\n",
    "    \"address_city\": \"City_a\",\n",
    "    \"address_state\": \"State_xxx\",\n",
    "    \"address_zipcode\": 10000\n",
    "}\n",
    "```\n",
    "\n",
    "## 引入项目依赖\n",
    "\n",
    "* 在 Jupyter 中，使用 `%%configure` 来配置饮用外部包。\n",
    "* 在 spark-shell 中，命令为 `pyspark --packages org.apache.spark:spark-streaming-kinesis-asl_2.11:2.4.2`\n",
    "* 通过 spark-submit 来提交，命令为 `spark-submit --packages org.apache.spark:spark-streaming-kinesis-asl_2.11:2.4.2 s3://<path-to-the-script-file-in-s3>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.jars\": \"s3://shiheng-poc/jars/.ivy2/jars/org.apache.spark_spark-streaming-kinesis-asl_2.11-2.4.0.jar,s3://shiheng-poc/jars/.ivy2/jars/com.amazonaws_aws-java-sdk-core-1.11.271.jar,s3://shiheng-poc/jars/.ivy2/jars/com.amazonaws_aws-java-sdk-s3-1.11.271.jar,s3://shiheng-poc/jars/.ivy2/jars/com.amazonaws_amazon-kinesis-client-1.8.10.jar\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.amazonaws.auth.DefaultAWSCredentialsProviderChain\n",
    "import com.amazonaws.services.kinesis.AmazonKinesisClient\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.streaming.kinesis.KinesisInitialPositions.Latest\n",
    "import org.apache.spark.streaming.kinesis.KinesisInputDStream\n",
    "import org.apache.spark.streaming._\n",
    "import scala.util.parsing.json._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.types.{StructType,ArrayType}\n",
    "import org.joda.time.DateTime\n",
    "\n",
    "val appName     = \"shiHengKinesisSparkApp\"\n",
    "val streamName  = \"shiheng-orders\"\n",
    "val endpointUrl = \"https://kinesis.cn-northwest-1.amazonaws.com.cn\"\n",
    "val regionName  = \"cn-northwest-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义数据转化\n",
    "\n",
    "以下定义了方法，将 nested JSON 进行铺平。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flattenDataframe(df: DataFrame): DataFrame = {\n",
    "    val fields = df.schema.fields\n",
    "    val fieldNames = fields.map(x => x.name)\n",
    "    val length = fields.length\n",
    "    for(i <- 0 to fields.length-1){\n",
    "        val field = fields(i)\n",
    "        val fieldtype = field.dataType\n",
    "        val fieldName = field.name\n",
    "        fieldtype match {\n",
    "            case arrayType: ArrayType =>\n",
    "                val fieldNamesExcludingArray = fieldNames.filter(_!=fieldName)\n",
    "                val fieldNamesAndExplode = fieldNamesExcludingArray ++ Array(s\"explode_outer($fieldName) as $fieldName\")\n",
    "                // val fieldNamesToSelect = (fieldNamesExcludingArray ++ Array(s\"$fieldName.*\"))\n",
    "                val explodedDf = df.selectExpr(fieldNamesAndExplode:_*)\n",
    "                return flattenDataframe(explodedDf)\n",
    "            case structType: StructType =>\n",
    "                val childFieldnames = structType.fieldNames.map(childname => fieldName +\".\"+childname)\n",
    "                val newfieldNames = fieldNames.filter(_!= fieldName) ++ childFieldnames\n",
    "                val renamedcols = newfieldNames.map(x => (col(x.toString()).as(x.toString().replace(\".\", \"_\"))))\n",
    "                val explodedf = df.select(renamedcols:_*)\n",
    "                return flattenDataframe(explodedf)\n",
    "            case _ =>\n",
    "            }\n",
    "        }\n",
    "        df\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义 unix 时间转化成 `yyyyMMddHHmm` 格式的 UDF 方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val unixToDT = udf{(ordertime:Long) => new DateTime(ordertime * 1000).toDateTime.toString(\"yyyyMMddHH\").toLong}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置 Kinesis Client 的授权方式, 这里回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val credentials = new DefaultAWSCredentialsProviderChain().getCredentials()\n",
    "require(credentials != null, \"No AWS credentials found. Please specify credentials using one of the methods specified \" +\n",
    "  \"in http://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/credentials.html\")\n",
    "val kinesisClient = new AmazonKinesisClient(credentials)\n",
    "kinesisClient.setEndpoint(endpointUrl)\n",
    "val numShards = kinesisClient.describeStream(streamName).getStreamDescription.getShards().size()\n",
    "val numStreams = numShards\n",
    "\n",
    "// Spark Streaming interval\n",
    "val batchInterval = Milliseconds(60000)\n",
    "val kinesisCheckpointInterval = batchInterval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开启 Streamingm 读取 Kiensis 的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ssc = new StreamingContext(sc, batchInterval)\n",
    "// ssc.checkpoint(\"/tmp/checkpoint\") // no checkpoint can be set, or you'll get error\n",
    "\n",
    "val kinesisStreams = (0 until numStreams).map { i =>\n",
    "  KinesisInputDStream.builder\n",
    "    .streamingContext(ssc)\n",
    "    .streamName(streamName)\n",
    "    .endpointUrl(endpointUrl)\n",
    "    .regionName(regionName)\n",
    "    .initialPosition(new Latest())\n",
    "    .checkpointAppName(appName)\n",
    "    .checkpointInterval(kinesisCheckpointInterval)\n",
    "    .storageLevel(StorageLevel.MEMORY_AND_DISK_2)\n",
    "    .build()\n",
    "}\n",
    "\n",
    "val unionStreams = ssc.union(kinesisStreams)\n",
    "val sqlContext = SparkSession.builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 输出到 S3 中， 根据订单时间进行分区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unionStreams.foreachRDD ((rdd: RDD[Array[Byte]], time: Time) => {\n",
    "  println(\"**********************************************\")\n",
    "  println(rdd.count)\n",
    "  println(\"rdd isempty:\" + rdd.isEmpty)\n",
    "  if (rdd.isEmpty()) {\n",
    "    println(\"No data input!!!\")\n",
    "  } else {\n",
    "    val lines   = rdd.map(byteArray => new String(byteArray))//.collect()//.toList\n",
    "    val linesDF = sqlContext.read.json(lines)\n",
    "    // linesDF.show()\n",
    "    // linesDF.printSchema\n",
    "    val linesFlattenDF = flattenDataframe(linesDF)\n",
    "    val linesResultDF  = linesFlattenDF.withColumn(\"ot\", unixToDT(linesFlattenDF(\"ordertime\")))\n",
    "    // linesResultDF.show()\n",
    "    \n",
    "    // set mode to append in case `analysisexception path already exists`\n",
    "    linesResultDF.write.mode(\"append\").partitionBy(\"ot\").csv(\"s3://shiheng-poc/ss-kinesis-scala/\")\n",
    "  }\n",
    "  println(\"**********************************************\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开启 Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 退出程序\n",
    "\n",
    "1. 登录 EMR master node\n",
    "2. `yarn application -kill <application-id>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验结束\n",
    "\n",
    "打开 S3, 查看我们生成的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
