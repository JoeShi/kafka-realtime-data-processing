{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming 读取 Kinesis 数据 (scala)\n",
    "\n",
    "Kinesis中的原数据，目前 Kinesis 中不断有数据注入。\n",
    "```json\n",
    "{\n",
    "    \"ordertime\": 1573573055,\n",
    "    \"orderid\": 23,\n",
    "    \"itemid\": \"Item_1231231\",\n",
    "    \"orderunits\": 15,\n",
    "    \"address\": {\n",
    "        \"city\": \"City_a\",\n",
    "        \"state\": \"State_xxx\",\n",
    "        \"zipcode\": 10000\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "最终目标为\n",
    "1. Kinesis 中的数据落到 S3 中\n",
    "2. 根据 **ordertime** 这个字段进行分区，例如 `ot=2019101123/`\n",
    "3. 落盘 S3 的数据需要进行铺平, 目标格式如下：\n",
    "```json\n",
    "{\n",
    "    \"ordertime\": 1573573055,\n",
    "    \"orderid\": 23,\n",
    "    \"itemid\": \"Item_1231231\",\n",
    "    \"orderunits\": 15,\n",
    "    \"address_city\": \"City_a\",\n",
    "    \"address_state\": \"State_xxx\",\n",
    "    \"address_zipcode\": 10000\n",
    "}\n",
    "```\n",
    "\n",
    "## 引入项目依赖\n",
    "\n",
    "* 在 Jupyter 中，使用 `%%configure` 来配置饮用外部包。\n",
    "* 通过 spark-submit 来提交，命令为 `spark-submit --packages org.apache.spark:spark-streaming-kinesis-asl_2.11:2.4.4 --class ShiHengKinesisSparkApp s3://<path-to-the-jar-file-in-s3.jar>`\n",
    "\n",
    "## 创建目录结构\n",
    "\n",
    "```shell\n",
    "$ find .\n",
    ".\n",
    "./build.sbt\n",
    "./src\n",
    "./src/main\n",
    "./src/main/scala\n",
    "./src/main/scala/ShiHengKinesisSparkApp.scala\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编写配置文件\n",
    "\n",
    "```shell\n",
    "$ cat ./build.sbt\n",
    "name := \"ShiHengKinesisSparkApp\"\n",
    "version := \"1.0\"\n",
    "scalaVersion := \"2.11.12\"\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-streaming-kinesis-asl\" % \"2.4.4\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，scalaVersion, libraryDependencies的版本号可以通过下方命令查询："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "$ spark-shell --version\n",
    "```\n",
    "    Welcome to\n",
    "          ____              __\n",
    "         / __/__  ___ _____/ /__\n",
    "        _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "       /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.4\n",
    "          /_/\n",
    "\n",
    "    Using Scala version 2.11.12, OpenJDK 64-Bit Server VM, 1.8.0_222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// src/main/scala/ShiHengKinesisSparkApp.scala\n",
    "import com.amazonaws.auth.DefaultAWSCredentialsProviderChain\n",
    "import com.amazonaws.services.kinesis.AmazonKinesisClient\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.streaming.kinesis.KinesisInitialPositions.Latest\n",
    "import org.apache.spark.streaming.kinesis.KinesisInputDStream\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.types.{StructType,ArrayType}\n",
    "import org.joda.time.DateTime\n",
    "import scala.util.parsing.json._\n",
    "\n",
    "object ShiHengKinesisSparkApp {\n",
    "    def main(args: Array[String]) {\n",
    "        def flattenDataframe(df: DataFrame): DataFrame = {\n",
    "            val fields = df.schema.fields\n",
    "            val fieldNames = fields.map(x => x.name)\n",
    "            val length = fields.length\n",
    "            for(i <- 0 to fields.length-1){\n",
    "                val field = fields(i)\n",
    "                val fieldtype = field.dataType\n",
    "                val fieldName = field.name\n",
    "                fieldtype match {\n",
    "                    case arrayType: ArrayType =>\n",
    "                        val fieldNamesExcludingArray = fieldNames.filter(_!=fieldName)\n",
    "                        val fieldNamesAndExplode = fieldNamesExcludingArray ++ Array(s\"explode_outer($fieldName) as $fieldName\")\n",
    "                        // val fieldNamesToSelect = (fieldNamesExcludingArray ++ Array(s\"$fieldName.*\"))\n",
    "                        val explodedDf = df.selectExpr(fieldNamesAndExplode:_*)\n",
    "                        return flattenDataframe(explodedDf)\n",
    "                    case structType: StructType =>\n",
    "                        val childFieldnames = structType.fieldNames.map(childname => fieldName +\".\"+childname)\n",
    "                        val newfieldNames = fieldNames.filter(_!= fieldName) ++ childFieldnames\n",
    "                        val renamedcols = newfieldNames.map(x => (col(x.toString()).as(x.toString().replace(\".\", \"_\"))))\n",
    "                        val explodedf = df.select(renamedcols:_*)\n",
    "                        return flattenDataframe(explodedf)\n",
    "                    case _ =>\n",
    "                    }\n",
    "                }\n",
    "                df\n",
    "            }\n",
    "\n",
    "        val unixToDT = udf{(ordertime:Long) => new DateTime(ordertime * 1000).toDateTime.toString(\"yyyyMMddHHmm\").toLong}\n",
    "\n",
    "        val appName     = \"shiHengKinesisSparkApp\"\n",
    "        val streamName  = \"orders\"\n",
    "        val endpointUrl = \"https://kinesis.cn-northwest-1.amazonaws.com.cn\"\n",
    "        val regionName  = \"cn-northwest-1\"\n",
    "        val credentials = new DefaultAWSCredentialsProviderChain().getCredentials()\n",
    "        require(credentials != null, \"No AWS credentials found. Please specify credentials using one of the methods specified \" +\n",
    "          \"in http://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/credentials.html\")\n",
    "        val kinesisClient = new AmazonKinesisClient(credentials)\n",
    "        kinesisClient.setEndpoint(endpointUrl)\n",
    "        val numShards   = kinesisClient.describeStream(streamName).getStreamDescription.getShards().size()\n",
    "        val numStreams  = numShards\n",
    "\n",
    "        // Spark Streaming interval\n",
    "        val batchInterval = Milliseconds(5000)\n",
    "        val kinesisCheckpointInterval = batchInterval\n",
    "\n",
    "        val sparkConf   = new SparkConf().setAppName(appName) // for spark-submit\n",
    "        // val sparkConf   = sc // for spark-shell\n",
    "\n",
    "        val ssc = new StreamingContext(sparkConf, batchInterval)\n",
    "        // ssc.checkpoint(\"/tmp/checkpoint\") // no checkpoint can be set, or you'll get error\n",
    "\n",
    "        val kinesisStreams = (0 until numStreams).map { i =>\n",
    "          KinesisInputDStream.builder\n",
    "            .streamingContext(ssc)\n",
    "            .streamName(streamName)\n",
    "            .endpointUrl(endpointUrl)\n",
    "            .regionName(regionName)\n",
    "            .initialPosition(new Latest())\n",
    "            .checkpointAppName(appName)\n",
    "            .checkpointInterval(kinesisCheckpointInterval)\n",
    "            .storageLevel(StorageLevel.MEMORY_AND_DISK_2)\n",
    "            .build()\n",
    "        }\n",
    "\n",
    "        val unionStreams = ssc.union(kinesisStreams)\n",
    "        val sqlContext = SparkSession.builder().getOrCreate()\n",
    "\n",
    "        unionStreams.foreachRDD ((rdd: RDD[Array[Byte]], time: Time) => {\n",
    "          println(\"**********************************************\")\n",
    "          println(rdd.count)\n",
    "          println(\"rdd isempty:\" + rdd.isEmpty)\n",
    "          if (rdd.isEmpty()) {\n",
    "            println(\"No data input!!!\")\n",
    "          } else {\n",
    "            val lines   = rdd.map(byteArray => new String(byteArray))//.collect()//.toList\n",
    "            val linesDF = sqlContext.read.json(lines)\n",
    "            // linesDF.show()\n",
    "            // linesDF.printSchema\n",
    "            val linesFlattenDF = flattenDataframe(linesDF)\n",
    "            val linesResultDF  = linesFlattenDF.withColumn(\"ot\", unixToDT(linesFlattenDF(\"ordertime\")))\n",
    "            // linesResultDF.show()\n",
    "            \n",
    "            // set mode to append in case `analysisexception path already exists`\n",
    "            linesResultDF.write.mode(\"append\").partitionBy(\"ot\").csv(\"s3://joeshi-poc/binc/parted-prod/\")\n",
    "          }\n",
    "          println(\"**********************************************\")\n",
    "        })\n",
    "        ssc.start()\n",
    "        ssc.awaitTermination()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编译Scala脚本\n",
    "\n",
    "[参考](https://medium.com/@tedherman/compile-scala-on-emr-cb77610559f0)\n",
    "\n",
    "### 安装SBT\n",
    "\n",
    "这里使用SBT来编译脚本, 可以在[Github](https://github.com/sbt/sbt/releases/)找到最新的版本。\n",
    "\n",
    "```shell\n",
    "wget https://github.com/sbt/sbt/releases/download/v1.3.3/sbt-1.3.3.tgz\n",
    "```\n",
    "\n",
    "### 解压缩并配置环境变量\n",
    "\n",
    "```shell\n",
    "tar -xf sbt-1.3.3.tgz\n",
    "export PATH=$PATH:`pwd`/sbt/bin\n",
    "```\n",
    "\n",
    "### link lib文件夹\n",
    "\n",
    "需要创建软链接指向spark的lib库，否则会导致编译失败。\n",
    "```shell\n",
    "cd <project/path>\n",
    "ln -s /usr/lib/spark/jars lib\n",
    "```\n",
    "\n",
    "### 编译并打包\n",
    "\n",
    "```shell\n",
    "sbt compile\n",
    "sbt package\n",
    "```\n",
    "\n",
    "## 提交脚本\n",
    "\n",
    "打包后的文件存在`target/scala-2.11/`中。[参考](https://spark.apache.org/docs/latest/quick-start.html#self-contained-applications)\n",
    "\n",
    "```shell\n",
    "spark-submit --packages org.apache.spark:spark-streaming-kinesis-asl_2.11:2.4.4 target/scala-2.11/shihengkinesissparkapp_2.11-1.0.jar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 退出程序\n",
    "\n",
    "使用 yarn 退出\n",
    "1. 登录 EMR master node\n",
    "2. `yarn application -kill <application-id>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验结束\n",
    "\n",
    "打开 S3, 查看我们生成的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
