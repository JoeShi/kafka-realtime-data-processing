{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.jars\": \"s3://shiheng-poc/jars/.ivy2/jars/org.apache.spark_spark-streaming-kinesis-asl_2.11-2.4.0.jar,s3://shiheng-poc/jars/.ivy2/jars/com.amazonaws_aws-java-sdk-core-1.11.271.jar,s3://shiheng-poc/jars/.ivy2/jars/com.amazonaws_aws-java-sdk-s3-1.11.271.jar,s3://shiheng-poc/jars/.ivy2/jars/com.amazonaws_amazon-kinesis-client-1.8.10.jar\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//         \"spark.jars.ivy\": \"/home/hadoop/.ivy2\",\n",
    "//         \"spark.jars.packages\": \"org.apache.spark:spark-streaming-kinesis-asl_2.11:2.4.0\"\n",
    "//         \"spark.executor.extraClassPath\": \"s3://shiheng-poc/jars/.ivy2/jars/\",\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>59</td><td>application_1573529792134_0087</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-1-129.cn-northwest-1.compute.internal:20888/proxy/application_1573529792134_0087/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-2-21.cn-northwest-1.compute.internal:8042/node/containerlogs/container_1573529792134_0087_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "import com.amazonaws.auth.DefaultAWSCredentialsProviderChain\n",
      "import com.amazonaws.services.kinesis.AmazonKinesisClient\n",
      "import org.apache.log4j.{Level, Logger}\n",
      "import org.apache.spark.SparkConf\n",
      "import org.apache.spark.rdd.RDD\n",
      "import org.apache.spark.sql.SparkSession\n",
      "import org.apache.spark.storage.StorageLevel\n",
      "import org.apache.spark.streaming.kinesis.KinesisInitialPositions.Latest\n",
      "import org.apache.spark.streaming.kinesis.KinesisInputDStream\n",
      "import org.apache.spark.streaming._\n",
      "import scala.util.parsing.json._\n",
      "import org.apache.spark.sql.DataFrame\n",
      "import org.apache.spark.sql.types.{StructType, ArrayType}\n",
      "import org.joda.time.DateTime\n",
      "flattenDataframe: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
      "unixToDT: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,LongType,Some(List(LongType)))\n",
      "appName: String = shiHengKinesisSparkApp\n",
      "streamName: String = shiheng-orders\n",
      "endpointUrl: String = https://kinesis.cn-northwest-1.amazonaws.com.cn\n",
      "regionName: String = cn-northwest-1\n",
      "credentials: com.amazonaws.auth.AWSCredentials = com.amazonaws.auth.BasicSessionCredentials@30ec2f54\n",
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "kinesisClient: com.amazonaws.services.kinesis.AmazonKinesisClient = com.amazonaws.services.kinesis.AmazonKinesisClient@743c02a5\n",
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "numShards: Int = 1\n",
      "numStreams: Int = 1\n",
      "batchInterval: org.apache.spark.streaming.Duration = 5000 ms\n",
      "kinesisCheckpointInterval: org.apache.spark.streaming.Duration = 5000 ms\n",
      "sparkConf: org.apache.spark.SparkContext = org.apache.spark.SparkContext@7ae5f6c6\n"
     ]
    }
   ],
   "source": [
    "import com.amazonaws.auth.DefaultAWSCredentialsProviderChain\n",
    "import com.amazonaws.services.kinesis.AmazonKinesisClient\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.streaming.kinesis.KinesisInitialPositions.Latest\n",
    "import org.apache.spark.streaming.kinesis.KinesisInputDStream\n",
    "import org.apache.spark.streaming._\n",
    "import scala.util.parsing.json._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.types.{StructType,ArrayType}\n",
    "import org.joda.time.DateTime\n",
    "\n",
    "def flattenDataframe(df: DataFrame): DataFrame = {\n",
    "    val fields = df.schema.fields\n",
    "    val fieldNames = fields.map(x => x.name)\n",
    "    val length = fields.length\n",
    "    for(i <- 0 to fields.length-1){\n",
    "        val field = fields(i)\n",
    "        val fieldtype = field.dataType\n",
    "        val fieldName = field.name\n",
    "        fieldtype match {\n",
    "            case arrayType: ArrayType =>\n",
    "                val fieldNamesExcludingArray = fieldNames.filter(_!=fieldName)\n",
    "                val fieldNamesAndExplode = fieldNamesExcludingArray ++ Array(s\"explode_outer($fieldName) as $fieldName\")\n",
    "                // val fieldNamesToSelect = (fieldNamesExcludingArray ++ Array(s\"$fieldName.*\"))\n",
    "                val explodedDf = df.selectExpr(fieldNamesAndExplode:_*)\n",
    "                return flattenDataframe(explodedDf)\n",
    "            case structType: StructType =>\n",
    "                val childFieldnames = structType.fieldNames.map(childname => fieldName +\".\"+childname)\n",
    "                val newfieldNames = fieldNames.filter(_!= fieldName) ++ childFieldnames\n",
    "                val renamedcols = newfieldNames.map(x => (col(x.toString()).as(x.toString().replace(\".\", \"_\"))))\n",
    "                val explodedf = df.select(renamedcols:_*)\n",
    "                return flattenDataframe(explodedf)\n",
    "            case _ =>\n",
    "            }\n",
    "        }\n",
    "        df\n",
    "    }\n",
    "\n",
    "val unixToDT = udf{(ordertime:Long) => new DateTime(ordertime * 1000).toDateTime.toString(\"yyyyMMddHHmm\").toLong}\n",
    "\n",
    "val appName     = \"shiHengKinesisSparkApp\"\n",
    "val streamName  = \"shiheng-orders\"\n",
    "val endpointUrl = \"https://kinesis.cn-northwest-1.amazonaws.com.cn\"\n",
    "val regionName  = \"cn-northwest-1\"\n",
    "\n",
    "val credentials = new DefaultAWSCredentialsProviderChain().getCredentials()\n",
    "require(credentials != null, \"No AWS credentials found. Please specify credentials using one of the methods specified \" +\n",
    "  \"in http://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/credentials.html\")\n",
    "val kinesisClient = new AmazonKinesisClient(credentials)\n",
    "kinesisClient.setEndpoint(endpointUrl)\n",
    "val numShards = kinesisClient.describeStream(streamName).getStreamDescription.getShards().size()\n",
    "\n",
    "val numStreams = numShards\n",
    "\n",
    "// Spark Streaming interval\n",
    "val batchInterval = Milliseconds(5000)\n",
    "val kinesisCheckpointInterval = batchInterval\n",
    "\n",
    "// val regionName = getRegion.getRegionNameByEndpoint(endpointUrl)\n",
    "val sparkConf   = sc //new SparkConf().setAppName(appName)\n",
    "//.setMaster(\"local[*]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@5da8199e\n",
      "kinesisStreams: scala.collection.immutable.IndexedSeq[org.apache.spark.streaming.kinesis.KinesisInputDStream[Array[Byte]]] = Vector(org.apache.spark.streaming.kinesis.KinesisInputDStream@5f4d042f)\n",
      "unionStreams: org.apache.spark.streaming.dstream.DStream[Array[Byte]] = org.apache.spark.streaming.dstream.UnionDStream@5fe43376\n",
      "sqlContext: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@544b543c\n",
      "warning: there was one deprecation warning; re-run with -deprecation for details\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val ssc = new StreamingContext(sparkConf, batchInterval)\n",
    "// ssc.checkpoint(\"/tmp/checkpoint\") // no checkpoint can be set, or you'll get error\n",
    "\n",
    "val kinesisStreams = (0 until numStreams).map { i =>\n",
    "  KinesisInputDStream.builder\n",
    "    .streamingContext(ssc)\n",
    "    .streamName(streamName)\n",
    "    .endpointUrl(endpointUrl)\n",
    "    .regionName(regionName)\n",
    "    .initialPosition(new Latest())\n",
    "    .checkpointAppName(appName)\n",
    "    .checkpointInterval(kinesisCheckpointInterval)\n",
    "    .storageLevel(StorageLevel.MEMORY_AND_DISK_2)\n",
    "    .build()\n",
    "}\n",
    "\n",
    "val unionStreams = ssc.union(kinesisStreams)\n",
    "val sqlContext = SparkSession.builder().getOrCreate()\n",
    "\n",
    "unionStreams.foreachRDD ((rdd: RDD[Array[Byte]], time: Time) => {\n",
    "  println(\"**********************************************\")\n",
    "  println(rdd.count)\n",
    "  println(\"rdd isempty:\" + rdd.isEmpty)\n",
    "  if (rdd.isEmpty()) {\n",
    "    println(\"No data input!!!\")\n",
    "  } else {\n",
    "    val lines   = rdd.map(byteArray => new String(byteArray))//.collect()//.toList\n",
    "    val linesDF = sqlContext.read.json(lines)\n",
    "    // linesDF.show()\n",
    "    // linesDF.printSchema\n",
    "    val linesFlattenDF = flattenDataframe(linesDF)\n",
    "    val linesResultDF  = linesFlattenDF.withColumn(\"ot\", unixToDT(linesFlattenDF(\"ordertime\")))\n",
    "    // linesResultDF.show()\n",
    "    \n",
    "    // set mode to append in case `analysisexception path already exists`\n",
    "    linesResultDF.write.mode(\"append\").partitionBy(\"ot\").csv(\"s3://shiheng-poc/binc/parted-prod/\")\n",
    "  }\n",
    "  println(\"**********************************************\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// to stop executing ssc, click Kernel -> Shutdown in the toolbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### 下方为测试代码请忽略 ###\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>20</td><td>application_1573529792134_0036</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-1-129.cn-northwest-1.compute.internal:20888/proxy/application_1573529792134_0036/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-2-21.cn-northwest-1.compute.internal:8042/node/containerlogs/container_1573529792134_0036_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "import spark.implicits._\n",
      "jsonStr: String = { \"metadata\": { \"key\": 84896, \"value\": 54 }}\n",
      "df: org.apache.spark.sql.DataFrame = [metadata: struct<key: bigint, value: bigint>]\n"
     ]
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "val jsonStr = \"\"\"{ \"metadata\": { \"key\": 84896, \"value\": 54 }},{ \"metadata\": { \"key\": 84896, \"value\": 54 }}\"\"\"\n",
    "val df = spark.read.json(Seq(jsonStr).toDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|   metadata|\n",
      "+-----------+\n",
      "|[84896, 54]|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsonStr: String = { \"metadata\": { \"key\": 84896, \"value\": 54 }}\n",
      "df: org.apache.spark.sql.DataFrame = [metadata: struct<key: bigint, value: bigint>]\n",
      "+-----------+\n",
      "|   metadata|\n",
      "+-----------+\n",
      "|[84896, 54]|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val jsonStr = \"\"\"{ \"metadata\": { \"key\": 84896, \"value\": 54 }}\"\"\"\n",
    "val df = spark.read.json(Seq(jsonStr).toDS)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.SparkSession\n",
      "events: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[69] at parallelize at <console>:56\n",
      "sqlContext: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7e7af19b\n",
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "df: org.apache.spark.sql.DataFrame = [action: string, timestamp: string]\n",
      "+------+----------+\n",
      "|action| timestamp|\n",
      "+------+----------+\n",
      "|create|1452121277|\n",
      "|create|1452121277|\n",
      "|create|          |\n",
      "|create|      null|\n",
      "|create|      null|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "// val s_2 = jsonStr + \"\\n\" + jsonStr\n",
    "val events = sc.parallelize(\n",
    "  \"\"\"{\"action\":\"create\",\"timestamp\":1452121277}\"\"\" ::\n",
    "  \"\"\"{\"action\":\"create\",\"timestamp\":\"1452121277\"}\"\"\" ::\n",
    "  \"\"\"{\"action\":\"create\",\"timestamp\":\"\"}\"\"\" ::\n",
    "  \"\"\"{\"action\":\"create\",\"timestamp\":null}\"\"\" ::\n",
    "  \"\"\"{\"action\":\"create\",\"timestamp\":\"null\"}\"\"\" ::\n",
    "  Nil\n",
    ")\n",
    "val sqlContext = SparkSession.builder().getOrCreate()\n",
    "// val ssss = sc.parallelize(s_2)\n",
    "val df = sqlContext.read.json(events)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strList: List[String] = List()\n",
      "jsonString1: String = {\"ID\" : \"111\",\"NAME\":\"Arkay\",\"LOC\":\"Pune\"}\n",
      "jsonString2: String = {\"ID\" : \"222\",\"NAME\":\"DineshS\",\"LOC\":\"PCMC\"}\n",
      "strList: List[String] = List({\"ID\" : \"111\",\"NAME\":\"Arkay\",\"LOC\":\"Pune\"})\n",
      "strList: List[String] = List({\"ID\" : \"111\",\"NAME\":\"Arkay\",\"LOC\":\"Pune\"}, {\"ID\" : \"222\",\"NAME\":\"DineshS\",\"LOC\":\"PCMC\"})\n",
      "pS: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[79] at parallelize at <console>:57\n",
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "df: org.apache.spark.sql.DataFrame = [ID: string, LOC: string ... 1 more field]\n",
      "+---+----+-------+\n",
      "| ID| LOC|   NAME|\n",
      "+---+----+-------+\n",
      "|111|Pune|  Arkay|\n",
      "|222|PCMC|DineshS|\n",
      "+---+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var strList = List.empty[String]\n",
    "var jsonString1 = \"\"\"{\"ID\" : \"111\",\"NAME\":\"Arkay\",\"LOC\":\"Pune\"}\"\"\"\n",
    "var jsonString2 = \"\"\"{\"ID\" : \"222\",\"NAME\":\"DineshS\",\"LOC\":\"PCMC\"}\"\"\"\n",
    "strList = strList :+ jsonString1\n",
    "strList = strList :+ jsonString2\n",
    "val pS = sc.parallelize(strList)\n",
    "val df = sqlContext.read.json(pS)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    }
   ],
   "source": [
    "val a = \n",
    "  Array(\n",
    "    Array(\"4580056797\", \"0\", \"2015-07-29 10:38:42\", \"0\", \"1\", \"1\"), \n",
    "    Array(\"4580056797\", \"0\", \"2015-07-29 10:38:42\", \"0\", \"1\", \"1\"))\n",
    "\n",
    "val rdd = sc.makeRDD(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
