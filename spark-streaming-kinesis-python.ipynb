{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark  Streaming 读取 Kinesis 数据\n",
    "\n",
    "Kinesis中的原数据，目前 Kinesis 中不断有数据注入。\n",
    "```json\n",
    "{\n",
    "    \"ordertime\": 1573573055,\n",
    "    \"orderid\": 23,\n",
    "    \"itemid\": \"Item_1231231\",\n",
    "    \"orderunits\": 15,\n",
    "    \"address\": {\n",
    "        \"city\": \"City_a\",\n",
    "        \"state\": \"State_xxx\",\n",
    "        \"zipcode\": 10000\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "最终目标为\n",
    "1. Kinesis 中的数据落到 S3 中\n",
    "2. 根据 **ordertime** 这个字段进行分区，例如 `ordertime=2019101123/`\n",
    "3. 落盘 S3 的数据需要进行铺平, 目标格式如下：\n",
    "```json\n",
    "{\n",
    "    \"ordertime\": 1573573055,\n",
    "    \"orderid\": 23,\n",
    "    \"itemid\": \"Item_1231231\",\n",
    "    \"orderunits\": 15,\n",
    "    \"address_city\": \"City_a\",\n",
    "    \"address_state\": \"State_xxx\",\n",
    "    \"address_zipcode\": 10000\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引入外部 package\n",
    "\n",
    "不确定下面的引入方法是否正确，参考文档[stack overflow](https://stackoverflow.com/questions/49302862/how-to-configure-jupyter-configure-with-multiple-packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{ \"conf\": {\"spark.jars.packages\": \"org.apache.spark:spark-streaming-kinesis-asl_2.11:2.4.0\" }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化 Spark Context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kinesis-asl_2.11:2.4.2 pyspark-shell'\n",
    "\n",
    "import json\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kinesis import KinesisUtils, InitialPositionInStream\n",
    "import time\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSparkSessionInstance(sparkConf):\n",
    "    if (\"sparkSessionSingletonInstance\" not in globals()):\n",
    "        globals()[\"sparkSessionSingletonInstance\"] = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(conf=sparkConf) \\\n",
    "            .getOrCreate()\n",
    "    return globals()[\"sparkSessionSingletonInstance\"]\n",
    "\n",
    "\n",
    "def convertTime(timestamp):\n",
    "    return time.strftime(\"%Y%m%d%H\", time.localtime(timestamp))\n",
    "\n",
    "\n",
    "def save2s3(rdd):\n",
    "    spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    rowRdd = rdd.map(lambda w: Row(ordertime=w[0],ordertime_raw=w[1],orderid=w[2],itemid=w[3],orderunits=w[4],address_city=w[5],address_state=w[6],address_zipcode=w[7]))\n",
    "    df = spark.createDataFrame(rowRdd)\n",
    "    resultDF = df.select(df.ordertime,df.ordertime_raw,df.orderid,df.itemid,df.orderunits,df.address_city,df.address_state,df.address_zipcode)\n",
    "    resultDF.createOrReplaceTempView(\"resultDF\")\n",
    "    resultDF.write.partitionBy(\"ordertime\").csv(path=\"s3n://shiheng-poc/maweijun/data2\",mode=\"append\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.SparkContext()\n",
    "\n",
    "# spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "ssc = StreamingContext(spark, 60)  # 每 60s 保存一次数据\n",
    "streamName = 'shiheng-orders'\n",
    "appName = 'SpartStreamingShiheng'\n",
    "endpointUrl = \"https://kinesis.cn-northwest-1.amazonaws.com.cn\"\n",
    "regionName = \"cn-northwest-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dstream = KinesisUtils.createStream(ssc, appName, streamName, endpointUrl, regionName, InitialPositionInStream.LATEST, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_rdd_stream = dstream.map(lambda x:(convertTime(json.loads(x)[\"ordertime\"]),json.loads(x)[\"ordertime\"],json.loads(x)[\"orderid\"],json.loads(x)[\"itemid\"],json.loads(x)[\"orderunits\"],json.loads(x)[\"address\"][\"city\"],json.loads(x)[\"address\"][\"state\"],json.loads(x)[\"address\"][\"zipcode\"]))\n",
    "\n",
    "py_rdd_stream.foreachRDD(save2s3)\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
