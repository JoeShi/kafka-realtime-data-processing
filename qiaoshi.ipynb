{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming 读取 Kinesis 数据\n",
    "\n",
    "Kinesis中的原数据，目前 Kinesis 中不断有数据注入。\n",
    "```json\n",
    "{\n",
    "    \"ordertime\": 1573573055,\n",
    "    \"orderid\": 23,\n",
    "    \"itemid\": \"Item_1231231\",\n",
    "    \"orderunits\": 15,\n",
    "    \"address\": {\n",
    "        \"city\": \"City_a\",\n",
    "        \"state\": \"State_xxx\",\n",
    "        \"zipcode\": 10000\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "最终目标为\n",
    "1. Kinesis 中的数据落到 S3 中\n",
    "2. 根据 **ordertime** 这个字段进行分区，例如 `ordertime=2019101123/`\n",
    "3. 落盘 S3 的数据需要进行铺平, 目标格式如下：\n",
    "```json\n",
    "{\n",
    "    \"ordertime\": 1573573055,\n",
    "    \"orderid\": 23,\n",
    "    \"itemid\": \"Item_1231231\",\n",
    "    \"orderunits\": 15,\n",
    "    \"address_city\": \"City_a\",\n",
    "    \"address_state\": \"State_xxx\",\n",
    "    \"address_zipcode\": 10000\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化 Spark Context\n",
    "\n",
    "这个 Jar 是 Structured Streaming 对于 Kinesis 的实现，基于开源项目 [kinesis-sql](https://github.com/qubole/kinesis-sql)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.jars.packages\": \"s3://shiheng-poc/jars/spark-sql-kinesis_2.11-2.4.0.jar\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.streaming._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val spark = SparkSession.\n",
    "builder.\n",
    "appName(\"ShiHengSparkStructuredSparking\").\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义 JSON 铺平的 UDF\n",
    "\n",
    "主要参考了 [How to flatten JSON in Spark Dataframe](https://www.24tutorials.com/spark/flatten-json-spark-dataframe/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenDataframe(df: DataFrame): DataFrame = {\n",
    "\n",
    "val fields = df.schema.fields\n",
    "val fieldNames = fields.map(x => x.name)\n",
    "val length = fields.length\n",
    "\n",
    "for(i <- 0 to fields.length-1){\n",
    "  val field = fields(i)\n",
    "  val fieldtype = field.dataType\n",
    "  val fieldName = field.name\n",
    "  fieldtype match {\n",
    "    case arrayType: ArrayType =>\n",
    "      val fieldNamesExcludingArray = fieldNames.filter(_!=fieldName)\n",
    "      val fieldNamesAndExplode = fieldNamesExcludingArray ++ Array(s\"explode_outer($fieldName) as $fieldName\")\n",
    "     // val fieldNamesToSelect = (fieldNamesExcludingArray ++ Array(s\"$fieldName.*\"))\n",
    "      val explodedDf = df.selectExpr(fieldNamesAndExplode:_*)\n",
    "      return flattenDataframe(explodedDf)\n",
    "    case structType: StructType =>\n",
    "      val childFieldnames = structType.fieldNames.map(childname => fieldName +\".\"+childname)\n",
    "      val newfieldNames = fieldNames.filter(_!= fieldName) ++ childFieldnames\n",
    "      val renamedcols = newfieldNames.map(x => (col(x.toString()).as(x.toString().replace(\".\", \"_\"))))\n",
    "     val explodedf = df.select(renamedcols:_*)\n",
    "      return flattenDataframe(explodedf)\n",
    "    case _ =>\n",
    "  }\n",
    "}\n",
    "df\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建 Kinesis 中数据的 Schema\n",
    "\n",
    "Kinesis 中读取的数据，对 `data` 字段进行 base64 decode, decode 完毕后，是一个标准的 JSON. 如下是它的 schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orderSchema = new StructType().\n",
    "add(\"ordertime\", StringType).\n",
    "add(\"orderid\", IntegerType).\n",
    "add(\"itemid\", StringType).\n",
    "add(\"orderunits\", IntegerType).\n",
    "add(\"address\", new StructType().\n",
    "    add(\"city\", StringType).\n",
    "    add(\"state\", StringType).\n",
    "    add(\"zipcode\", IntegerType)\n",
    "   )\n",
    "\n",
    "// Array byte 转化成 UTF-8 string\n",
    "val b2String = udf((payload: Array[Byte]) => new String(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取 Kinesis 中的数据\n",
    "\n",
    "\n",
    "Kinesis 中 读取到的原始数据的格式，其中 **data** 字段为实际内容，该字段进行了 base64 加密。\n",
    "```\n",
    "{\n",
    "    \"data\": \"base64 encoded content\",\n",
    "    \"streamName\": \"stream-name\",\n",
    "    \"partitionKey\": \"kinesis-partition-key\",\n",
    "    \"sequenceNumber\": \"seq-number\",\n",
    "    \"approximateArrivalTimestamp\": 1573573055\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val kinesis = spark.readStream.\n",
    "format(\"kinesis\").\n",
    "option(\"streamName\", \"shiheng-orders\").\n",
    "option(\"endpointUrl\", \"https://kinesis.cn-northwest-1.amazonaws.com.cn\").\n",
    "option(\"startingPosition\", \"LATEST\").\n",
    "option(\"maxFetchDuration\", \"30s\").\n",
    "option(\"fetchBufferSize\", \"100mb\").\n",
    "load\n",
    "\n",
    "kinesis.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val records = kinesis.select(from_json(b2String('data), orderSchema) as 'root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val records_2 = flattenDataframe(records.select(\"root.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_2.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val result = kinesis.selectExpr(\"lcase(CAST(data as STRING)) as word\").groupBy($\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
